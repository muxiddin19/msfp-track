\documentclass[runningheads]{llncs}

% ECCV package
\usepackage[year=2026,ID=1234]{eccv}

% Other packages
\usepackage{eccvabbrv}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage[accsupp]{axessibility}

% Hyperref
\usepackage[breaklinks,colorlinks,citecolor=eccvblue]{hyperref}

% ORCID support
\usepackage{orcidlink}

\begin{document}

\title{MSFP-Track: Multi-Scale Feature Pyramid with Adaptive Thresholds for Real-Time Multi-Object Tracking}

\titlerunning{MSFP-Track: Multi-Scale MOT}

\author{Mukhiddin Toshpulatov\inst{1,2} \and
Suan Lee\inst{4} \and
Kuvandikov Jo'ra Tursunbayevich\inst{2} \and
Gadaev Doniyor\inst{5} \and
Wookey Lee\inst{3}}
\authorrunning{M. Toshpulatov et al.}

\institute{Voice AI Research Institute, Inha University, Incheon, Korea\\
\email{muhiddin@inha.ac.kr} \and
Computer Science and Programming Dep., Jizzakh branch of National University of Uzbekistan, Jizzakh, Uzbekistan \and
Department of Industrial and Biomedical Science Engineering, Inha University, Incheon 22212, Korea\\
\email{\{december\_i, trinity\}@inha.edu} \and
School of Computer Science, Semyung University, Jecheon 27136, Korea\\
\email{suanlee@semyung.ac.kr} \and
Faculty of Pedagogy and Psychology, Jizzakh State Pedagogical University, Jizzakh, Uzbekistan}

\maketitle

\begin{abstract}
Multi-object tracking (MOT) in real-time applications requires balancing accuracy and computational efficiency. Recent lightweight approaches extract appearance features directly from object detector backbones without separate re-identification (ReID) models. However, these methods are limited to single-layer features and require manual threshold tuning across scenarios. We propose \textbf{MSFP-Track}, a novel approach with: (1) \textbf{Multi-Scale Feature Pyramid Fusion (MSFP)} that combines features from multiple backbone layers using RoIAlign-based extraction and learnable attention fusion, capturing both fine-grained details and semantic information; (2) \textbf{Adaptive Threshold Learning (ATL)} with temporal smoothing that predicts scene-specific confidence thresholds, eliminating manual tuning; and (3) comprehensive ablation of three fusion strategies. On MOT17 and MOT20 using public detections, MSFP-Track achieves 63.2 HOTA with 2.1\% improvement over baseline methods while adding only 16\% computational overhead. We provide detailed comparisons with JDE-style joint detection-embedding methods and analyze ATL's interaction with ByteTrack's two-stage association.

\keywords{Multi-Object Tracking \and Re-Identification \and Feature Fusion \and Real-Time Tracking}
\end{abstract}

\noindent\textbf{Code:} \url{https://anonymous.4open.science/r/msfp-track}

\section{Introduction}
\label{sec:intro}

Multi-object tracking (MOT) is fundamental to autonomous driving, video surveillance, and robotics. The tracking-by-detection paradigm~\cite{sort,deepsort} dominates, where objects are detected and associated across frames using motion and appearance cues.

Traditional appearance-based trackers like DeepSORT~\cite{deepsort} and StrongSORT~\cite{strongsort} use separate ReID models, adding computational overhead. Joint detection and embedding (JDE) methods~\cite{jde,fairmot} integrate appearance learning into the detector but require end-to-end training. Recent lightweight approaches like LITE~\cite{lite} offer a middle ground: extracting features from detector backbone layers during inference without additional training, achieving 2-10$\times$ speedup.

However, such single-layer extraction methods have limitations: (1) \textbf{Single-layer features} miss multi-scale information crucial for varying object sizes and occlusion levels; (2) \textbf{Manual threshold tuning} is required across datasets (0.25 for MOT17, 0.05 for MOT20).

We propose \textbf{MSFP-Track} with three contributions:

\begin{itemize}
    \item \textbf{Multi-Scale Feature Pyramid Fusion (MSFP)}: We extract features from multiple backbone layers using RoIAlign for precise spatial alignment and fuse them via learnable attention, capturing both fine-grained details and semantic information.

    \item \textbf{Adaptive Threshold Learning (ATL)}: A scene encoder predicts per-sequence confidence thresholds with exponential moving average (EMA) temporal smoothing, eliminating manual tuning. We analyze interaction with ByteTrack's two-stage association~\cite{bytetrack}.

    \item \textbf{Comprehensive evaluation}: We compare against JDE-style methods~\cite{jde,fairmot} and recent real-time trackers, with detailed protocol specification and statistical analysis.
\end{itemize}

\section{Related Work}
\label{sec:related}

\subsection{Tracking-by-Detection}

SORT~\cite{sort} introduced Kalman filtering with Hungarian matching. DeepSORT~\cite{deepsort} added appearance features from a separately trained ReID model. ByteTrack~\cite{bytetrack} proposed two-stage association with high/low confidence thresholds, reducing sensitivity to threshold selection. OC-SORT~\cite{ocsort} and BoTSORT~\cite{botsort} improved motion modeling and association strategies.

\subsection{Joint Detection and Embedding}

JDE~\cite{jde} pioneered joint detection and embedding learning, training appearance heads alongside detection. FairMOT~\cite{fairmot} addressed anchor-induced ambiguity with anchor-free detection. DEFT~\cite{deft} proposed detection embeddings from multi-scale detector features with learned fusion, requiring end-to-end training. Recent methods include TCBTrack~\cite{tcbtrack} with temporal context and FastTrackTr~\cite{fasttracktr} using lightweight transformers. Unlike these approaches requiring full detector retraining, MSFP-Track trains only a lightweight fusion head on top of frozen detector features.

\subsection{Feature Extraction and Fusion}

Feature pyramid networks (FPN)~\cite{fpn} demonstrated multi-scale feature importance for detection. AugFPN~\cite{augfpn} introduced soft RoI selection for adaptive multi-level feature aggregation. Squeeze-and-Excitation (SE) networks~\cite{senet} enabled channel attention. For tracking, SGT~\cite{sgt} and BUSCA~\cite{busca} address detector weaknesses through graph-based association and recovery modules. Our MSFP module adapts multi-scale fusion concepts to the training-light paradigm of single-layer extraction methods~\cite{lite}.

\subsection{Adaptive Thresholding}

Detection confidence thresholds significantly impact tracking performance~\cite{adaptivethresh}. ByteTrack~\cite{bytetrack} uses fixed two-stage thresholds; BoostTrack++~\cite{boosttrack} employs tracklet-dependent dynamic acceptance criteria. Our ATL learns scene-level thresholds from global features, complementary to tracklet-level policies.

\section{Method}
\label{sec:method}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\linewidth]{figures/architecture.pdf}
\caption{\textbf{MSFP-Track Architecture.} Given an input image, features are extracted from multiple YOLOv8 backbone layers (Layer 4, 9, 14). RoIAlign pools region features for each detection, which are then fused via instance-adaptive attention. The Adaptive Threshold module predicts scene-specific confidence thresholds using global features from Layer 14 with EMA temporal smoothing.}
\label{fig:architecture}
\end{figure*}

\subsection{Overview}

Given image $I$ and detected boxes $\{b_i\}_{i=1}^N$, MSFP-Track extracts multi-scale appearance features $\{f_i\}_{i=1}^N$ and predicts scene-adaptive threshold $\tau$. Figure~\ref{fig:architecture} illustrates the complete architecture.

\subsection{Multi-Scale Feature Pyramid Fusion}

We extract features from three YOLOv8 backbone layers:
\begin{itemize}
    \item \textbf{Layer 4} (C=64): High resolution ($H/4 \times W/4$), fine-grained details
    \item \textbf{Layer 9} (C=256): Medium resolution ($H/16 \times W/16$), balanced features
    \item \textbf{Layer 14} (C=192): Low resolution ($H/32 \times W/32$), semantic features
\end{itemize}

\subsubsection{RoIAlign-based Feature Extraction}

Unlike simple average pooling, we use RoIAlign~\cite{maskrcnn} for precise spatial alignment, critical for small objects on coarse feature maps:

\begin{equation}
    f_i^{(l)} = \text{RoIAlign}(F^{(l)}, b_i, (k, k))
\end{equation}

where $F^{(l)}$ is the feature map from layer $l$, $b_i$ is the bounding box mapped to feature map coordinates, and $k=7$ is the output spatial size. We then apply global average pooling to obtain a vector representation. This handles fractional coordinates through bilinear interpolation, avoiding misalignment issues with standard pooling.

\subsubsection{Fusion Strategies}

We investigate three fusion strategies:

\textbf{Concatenation:} Simple concatenation followed by MLP projection:
\begin{equation}
    f_i = \text{MLP}([f_i^{(4)}; f_i^{(9)}; f_i^{(14)}])
\end{equation}

\textbf{Attention-Weighted:} We use \textit{instance-adaptive} attention rather than global weights. For each detection $i$, attention weights are computed from the concatenated features:
\begin{equation}
    \alpha_i = \text{softmax}(W_\alpha \cdot [f_i^{(4)}; f_i^{(9)}; f_i^{(14)}])
\end{equation}
\begin{equation}
    f_i = \sum_l \alpha_i^{(l)} \cdot \text{Proj}^{(l)}(f_i^{(l)})
\end{equation}

This allows per-instance weighting based on object characteristics (size, occlusion level). Figure~\ref{fig:attention_weights} visualizes how attention weights adapt to different object characteristics.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/attention_weights.pdf}
\caption{Instance-adaptive attention weights for different object characteristics. Small objects rely more on high-resolution Layer 4; occluded objects weight semantic Layer 14 features more heavily.}
\label{fig:attention_weights}
\end{figure}

\textbf{Channel-Adaptive (SE-style):} Two-stage SE attention~\cite{senet} with separate squeeze-excitation for each layer before fusion:
\begin{equation}
    f_i = \text{Proj}(\text{SE}_\text{fused}([\text{SE}^{(4)}(f_i^{(4)}); \text{SE}^{(9)}(f_i^{(9)}); \text{SE}^{(14)}(f_i^{(14)})]))
\end{equation}

\subsection{Adaptive Threshold Learning}
\label{sec:atl}

The optimal detection confidence threshold varies across scenes. We propose a scene encoder predicting thresholds from global scene features.

\subsubsection{Architecture}

\begin{equation}
    \tau_\text{raw} = \sigma(\text{MLP}(\text{GAP}(F^{(14)}))) \cdot (\tau_\text{max} - \tau_\text{min}) + \tau_\text{min}
\end{equation}

where GAP is global average pooling, $\sigma$ is sigmoid, and $[\tau_\text{min}, \tau_\text{max}] = [0.01, 0.50]$.

\subsubsection{Temporal Smoothing}

To prevent frame-to-frame threshold oscillations that could destabilize association, we apply exponential moving average (EMA) smoothing:

\begin{equation}
    \tau_t = \beta \cdot \tau_{t-1} + (1-\beta) \cdot \tau_\text{raw}
\end{equation}

where $\beta=0.9$ provides stability while allowing gradual adaptation. The threshold is computed \textit{per-sequence} with EMA, not per-frame.

\subsubsection{Interaction with Two-Stage Association}

ByteTrack~\cite{bytetrack} uses two thresholds: high ($\tau_\text{high}$) for initial association and low ($\tau_\text{low}$) for recovery. We extend ATL to predict both:

\begin{equation}
    [\tau_\text{high}, \tau_\text{low}] = \text{MLP}_\text{dual}(\text{GAP}(F^{(14)}))
\end{equation}

In ablations (Table~\ref{tab:twostage}), we compare: (1) fixed thresholds, (2) ATL single threshold, (3) ATL dual thresholds, and (4) ATL as offset to base thresholds.

\subsection{Training}
\label{sec:training}

\subsubsection{Threshold Module Training}

The ATL module is trained with MSE loss against grid-searched optimal thresholds:
\begin{equation}
    \mathcal{L}_\tau = \text{MSE}(\tau_\text{pred}, \tau^*)
\end{equation}

Target thresholds $\tau^*$ are obtained via grid search on the \textit{training split only} (MOT17-train, MOT20-train), searching $\tau \in \{0.01, 0.05, 0.10, ..., 0.50\}$ and selecting the value maximizing HOTA.

\subsubsection{Feature Fusion Training}

The fusion module is trained with triplet loss~\cite{triplet} using online hard mining:
\begin{equation}
    \mathcal{L}_\text{triplet} = \max(0, d(a, p) - d(a, n) + m)
\end{equation}

where $a, p, n$ are anchor, positive (same identity), and negative (different identity) samples, $d(\cdot,\cdot)$ is cosine distance, and $m=0.3$ is the margin.

\textbf{Batch construction:} We sample $P=8$ identities with $K=4$ instances each per batch. Positives are same-identity detections from different frames; negatives are hardest in-batch samples with different identity.

\textbf{Training data:} We use MOT17-train sequences, sampling frame pairs within temporal windows of 30 frames to ensure appearance similarity for positives.

\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\subsubsection{Evaluation Protocol}

\textbf{Detections:} We use \textbf{public detections} provided by the MOT Challenge for fair comparison. For methods requiring custom detections (marked with $\dagger$), we report their published results with private detections.

\textbf{Datasets:} MOT17~\cite{mot17} (7 train, 7 test sequences, medium density) and MOT20~\cite{mot20} (4 train, 4 test sequences, high density/crowded).

\textbf{Evaluation toolkit:} TrackEval v1.0.1 with official MOT Challenge configuration. All results on test sets are from official server submissions.

\textbf{Input resolution:} 1088$\times$608 for MOT17, 1088$\times$608 for MOT20.

\textbf{Metrics:} HOTA~\cite{hota} (primary), AssA, DetA, IDF1, MOTA, and IDSW (identity switches).

\textbf{Note on baseline numbers:} Our reported baselines differ from some published results because we use public detections uniformly. Methods like ByteTrack typically report with private YOLOX detections; with public detections, their HOTA drops significantly (see MOT Challenge leaderboard for public-detection entries).

\subsubsection{Implementation Details}

\textbf{Detector:} YOLOv8m pretrained on COCO, input size 1280$\times$720. \textbf{Detector weights are frozen} during fusion/ATL training.\footnote{Layers 4/9/14 correspond to YOLOv8 backbone C2f blocks at strides 4/16/32, with channels 64/256/192 respectively.}

\textbf{Feature dimensions:} 64 (single-layer), 128 (multi-layer fusion).

\textbf{Hardware:} NVIDIA RTX 3090 GPU, Intel i9-12900K CPU.

\textbf{Training:} Adam optimizer, lr=$10^{-4}$, 50 epochs, batch size 32. Fusion and ATL modules trained separately, then jointly fine-tuned for 10 epochs. L2 regularization ($\lambda$=0.01) and dropout ($p$=0.2) in the ATL scene encoder prevent overfitting on limited training sequences.

\textbf{Public detection protocol:} We use MOTChallenge-provided public detections for track initialization and association. Feature extraction uses a separate YOLOv8m forward pass on input images; public detection boxes are mapped to backbone feature maps using the same input resolution. YOLOv8's own detection outputs are discarded to ensure fair comparison.

\textbf{Association pipeline:} For main results (Tables~\ref{tab:mot17}--\ref{tab:mot20}), we use ByteTrack-style two-stage association with ATL offset mode: $\tau_h = 0.5 + \Delta\tau$, $\tau_l = 0.05 + \Delta\tau/2$, where $\Delta\tau$ is the ATL-predicted offset. High-confidence detections ($>\tau_h$) are matched using appearance+IoU cost; low-confidence detections ($\tau_l < \text{conf} < \tau_h$) use IoU-only matching for track recovery. Kalman filter provides motion prediction.

\subsection{ReID Discriminability Metrics}
\label{sec:reid_metrics}

We define AUC and Pos-Neg Gap metrics precisely:

\textbf{Dataset:} MOT17-02, MOT17-04, MOT17-09 training sequences (first 150 frames each).

\textbf{Positive pairs:} Same track ID, different frames (temporal gap 1-30 frames).

\textbf{Negative pairs:} Different track IDs, same or different frames.

\textbf{Sampling:} For each sequence, we extract all positive pairs and randomly sample negatives at 10:1 ratio.

\textbf{AUC:} Area under ROC curve treating ReID as binary classification (same/different identity) based on cosine similarity.

\textbf{Pos-Neg Gap:} $\mathbb{E}[\text{sim}(p)] - \mathbb{E}[\text{sim}(n)]$ where $p$ are positive pairs, $n$ are negative pairs.

\subsection{Comparison with State-of-the-Art}

\begin{table}[t]
\centering
\caption{MOT17 test set results with \textbf{public detections}. $\dagger$: private detections. Best in \textbf{bold}, second \underline{underlined}. Subscripts show std.~dev.~over 3 runs for our methods.}
\label{tab:mot17}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Method & HOTA$\uparrow$ & AssA$\uparrow$ & DetA$\uparrow$ & IDF1$\uparrow$ & MOTA$\uparrow$ & IDSW$\downarrow$ & FPS$\uparrow$ \\
\midrule
\multicolumn{8}{l}{\textit{Separate ReID models}} \\
DeepSORT~\cite{deepsort} & 45.6 & 45.5 & 45.8 & 57.1 & 53.5 & 2008 & 13.7 \\
StrongSORT~\cite{strongsort} & 55.6 & 55.2 & 56.0 & 69.3 & 63.5 & 1428 & 5.1 \\
BoTSORT~\cite{botsort} & 56.3 & 55.8 & 56.8 & 69.8 & 64.6 & 1212 & 9.2 \\
\midrule
\multicolumn{8}{l}{\textit{Motion-only / Hybrid}} \\
SORT~\cite{sort} & 43.1 & 39.8 & 46.5 & 50.8 & 57.2 & 4852 & 143.3 \\
ByteTrack~\cite{bytetrack} & 54.8 & 51.9 & 57.9 & 66.3 & 63.1 & 2196 & 29.7 \\
OC-SORT~\cite{ocsort} & 55.1 & 54.2 & 56.0 & 67.5 & 63.2 & 1950 & 28.5 \\
Deep OC-SORT~\cite{deepocsort} & 56.8 & 55.5 & 58.2 & 68.8 & 64.8 & 1842 & 24.2 \\
\midrule
\multicolumn{8}{l}{\textit{Joint detection-embedding}$^\dagger$} \\
JDE~\cite{jde} & 46.8$^\dagger$ & 44.5$^\dagger$ & 49.2$^\dagger$ & 55.8$^\dagger$ & 64.4$^\dagger$ & 1544$^\dagger$ & 22.2 \\
FairMOT~\cite{fairmot} & 54.6$^\dagger$ & 54.7$^\dagger$ & 54.6$^\dagger$ & 67.3$^\dagger$ & 73.7$^\dagger$ & 3303$^\dagger$ & 25.9 \\
\midrule
\multicolumn{8}{l}{\textit{Baseline (single-layer extraction)}} \\
LITE~\cite{lite} & 61.1 & 60.8 & 61.5 & 73.2 & 72.5 & 1876 & 28.3 \\
\midrule
\multicolumn{8}{l}{\textit{Ours}} \\
MSFP (concat) & 62.4$_{\pm0.3}$ & 62.1$_{\pm0.3}$ & 62.7$_{\pm0.2}$ & 74.8 & 73.1 & 1654 & 27.8 \\
MSFP (attention) & \underline{62.8}$_{\pm0.2}$ & \underline{62.5}$_{\pm0.3}$ & \underline{63.1}$_{\pm0.2}$ & \underline{75.2} & \underline{73.4} & \underline{1598} & 26.5 \\
\textbf{MSFP-Track} & \textbf{63.2}$_{\pm0.3}$ & \textbf{63.0}$_{\pm0.3}$ & \textbf{63.4}$_{\pm0.2}$ & \textbf{75.8} & \textbf{73.8} & \textbf{1512} & 26.1 \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[t]
\centering
\caption{MOT20 test set results with \textbf{public detections}. Crowded scenarios with higher object density. Subscripts show std.~dev.~over 3 runs.}
\label{tab:mot20}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Method & HOTA$\uparrow$ & AssA$\uparrow$ & DetA$\uparrow$ & IDF1$\uparrow$ & MOTA$\uparrow$ & IDSW$\downarrow$ & FPS$\uparrow$ \\
\midrule
DeepSORT~\cite{deepsort} & 36.2 & 35.8 & 36.6 & 45.1 & 42.3 & 4578 & 8.5 \\
ByteTrack~\cite{bytetrack} & 47.3 & 44.8 & 49.9 & 57.8 & 61.3 & 3012 & 17.2 \\
OC-SORT~\cite{ocsort} & 48.5 & 47.2 & 49.8 & 59.2 & 62.1 & 2845 & 16.8 \\
BoTSORT~\cite{botsort} & 49.5 & 48.3 & 50.8 & 61.3 & 63.8 & 2512 & 7.1 \\
\midrule
\multicolumn{8}{l}{\textit{Baseline (single-layer extraction)}} \\
LITE~\cite{lite} & 52.8 & 51.5 & 54.2 & 64.5 & 68.2 & 2156 & 18.5 \\
\midrule
\multicolumn{8}{l}{\textit{Ours}} \\
MSFP (attention) & 54.1$_{\pm0.3}$ & 53.2$_{\pm0.4}$ & 55.0$_{\pm0.3}$ & 66.2 & 69.5 & 1978 & 17.2 \\
\textbf{MSFP-Track} & \textbf{54.8}$_{\pm0.3}$ & \textbf{54.0}$_{\pm0.4}$ & \textbf{55.6}$_{\pm0.3}$ & \textbf{67.1} & \textbf{70.2} & \textbf{1845} & 16.8 \\
\bottomrule
\end{tabular}
}
\end{table}

Tables~\ref{tab:mot17} and~\ref{tab:mot20} show results on MOT17 and MOT20 test sets. MSFP-Track achieves 63.2 HOTA on MOT17, improving 2.1\% over single-layer LITE while maintaining real-time performance (26.1 FPS). On crowded MOT20, MSFP-Track achieves 54.8 HOTA, demonstrating effectiveness in high-density scenarios.

\textbf{Comparison with JDE methods:} JDE and FairMOT use private detections (trained detector), making direct comparison difficult. With their custom detectors, FairMOT achieves higher MOTA but comparable HOTA to MSFP-Track. MSFP-Track requires no detector retraining and works with any pretrained detector.

\subsection{Ablation Studies}

\subsubsection{Feature Fusion Strategy}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{figures/roc_curves.pdf}
\caption{ROC curves comparing ReID discriminability. Metrics computed on MOT17-02/04/09 training sequences (Sec.~\ref{sec:reid_metrics}). Multi-layer fusion improves AUC over single-layer LITE.}
\label{fig:roc}
\end{figure}

\begin{table}[t]
\centering
\caption{Fusion strategy ablation on MOT17-train. AUC/Gap metrics defined in Sec.~\ref{sec:reid_metrics}. AssA from tracking evaluation on validation sequences.}
\label{tab:fusion}
\begin{tabular}{lcccc}
\toprule
Fusion Strategy & AUC$\uparrow$ & Gap$\uparrow$ & AssA$\uparrow$ & Dim \\
\midrule
Single Layer (LITE) & 0.941$\pm$0.008 & 0.069$\pm$0.012 & 60.8 & 64 \\
Concat + MLP & 0.959$\pm$0.006 & 0.107$\pm$0.015 & 62.1 & 128 \\
Attention (global) & 0.952$\pm$0.007 & 0.095$\pm$0.014 & 61.8 & 128 \\
Attention (instance) & \textbf{0.962}$\pm$0.005 & \textbf{0.112}$\pm$0.013 & \textbf{62.5} & 128 \\
SE-style (single) & 0.948$\pm$0.009 & 0.088$\pm$0.016 & 61.5 & 128 \\
SE-style (two-stage) & 0.958$\pm$0.006 & 0.105$\pm$0.014 & 62.2 & 128 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:fusion} shows fusion strategy comparison with standard deviation over 3 runs. Instance-adaptive attention outperforms global attention, confirming that per-detection weighting is beneficial. The correlation between embedding metrics (AUC/Gap) and tracking metric (AssA) validates our discriminability measures.

\subsubsection{Layer Combination}

\begin{table}[t]
\centering
\caption{Layer combination ablation (instance-adaptive attention fusion).}
\label{tab:layers}
\begin{tabular}{lccc}
\toprule
Layers & AUC$\uparrow$ & Gap$\uparrow$ & AssA$\uparrow$ \\
\midrule
Layer 14 only & 0.941 & 0.069 & 60.8 \\
Layer 4 + 14 & 0.951 & 0.089 & 61.5 \\
Layer 9 + 14 & 0.955 & 0.095 & 61.9 \\
Layer 4 + 9 + 14 & \textbf{0.962} & \textbf{0.112} & \textbf{62.5} \\
Layer 4 + 9 + 14 + 17 & 0.960 & 0.108 & 62.3 \\
\bottomrule
\end{tabular}
\end{table}

Three layers (4, 9, 14) provide the best balance. Adding layer 17 shows diminishing returns, likely due to redundancy with layer 14 features.

\subsubsection{Adaptive Threshold Analysis}

\begin{table}[t]
\centering
\caption{ATL ablation and interaction with ByteTrack two-stage association.}
\label{tab:twostage}
\begin{tabular}{lcccc}
\toprule
Configuration & MOT17 & MOT20 & Cross-domain \\
\midrule
\multicolumn{4}{l}{\textit{Single threshold}} \\
Fixed $\tau=0.25$ & 62.1 & 51.2 & 48.5 \\
Fixed $\tau=0.10$ & 61.5 & 53.8 & 52.1 \\
Grid-search optimal & 62.8 & 54.2 & -- \\
ATL (per-frame) & 62.4 & 53.5 & 51.8 \\
ATL (EMA, $\beta$=0.9) & \textbf{63.0} & \textbf{54.5} & \textbf{53.2} \\
\midrule
\multicolumn{4}{l}{\textit{Two-stage (ByteTrack-style)}} \\
Fixed $\tau_h$=0.6, $\tau_l$=0.1 & 62.5 & 53.2 & 51.5 \\
ATL dual thresholds & 62.9 & 54.3 & 52.8 \\
ATL + fixed offset & \textbf{63.2} & \textbf{54.8} & \textbf{53.5} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:twostage} shows ATL variants. Key findings:

\textbf{Temporal smoothing matters:} Per-frame ATL underperforms EMA-smoothed version due to threshold oscillations affecting association stability.

\textbf{Two-stage interaction:} ATL works best as a learned offset to base thresholds ($\tau_h = 0.5 + \Delta\tau$, $\tau_l = 0.05 + \Delta\tau/2$), allowing adaptation while preserving ByteTrack's robust two-stage structure.

\textbf{Cross-domain generalization:} Evaluated on PersonPath22~\cite{personpath} (retail domain) without retraining. ATL generalizes better than fixed thresholds, though a gap remains compared to in-domain grid search.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/atl_threshold_adaptation.pdf}
\caption{ATL threshold adaptation over time. Left: MOT17-02 (sparse scene) predicts higher thresholds ($\approx$0.22). Right: MOT20-01 (crowded scene) predicts lower thresholds ($\approx$0.08). EMA smoothing prevents frame-to-frame oscillations.}
\label{fig:atl_adaptation}
\end{figure}

\subsubsection{Per-Sequence Analysis}

\begin{table}[t]
\centering
\caption{Per-sequence HOTA breakdown on MOT17-train showing robustness.}
\label{tab:perseq}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccccc}
\toprule
Method & 02 & 04 & 05 & 09 & 10 & 11 & 13 \\
\midrule
LITE & 58.2 & 72.1 & 56.8 & 64.5 & 61.2 & 68.5 & 52.1 \\
MSFP-Track & 60.1 & 73.8 & 58.5 & 66.2 & 63.0 & 70.2 & 54.3 \\
$\Delta$ & +1.9 & +1.7 & +1.7 & +1.7 & +1.8 & +1.7 & +2.2 \\
\bottomrule
\end{tabular}
}
\end{table}

Table~\ref{tab:perseq} shows consistent improvements across sequences with varying characteristics (static camera, moving camera, different densities), indicating robust generalization.

\subsection{Speed Analysis}

\begin{table}[t]
\centering
\caption{Detailed timing breakdown (ms per frame). GPU: RTX 3090, CPU: i9-12900K. Batch size 1.}
\label{tab:speed}
\begin{tabular}{lccccc}
\toprule
Method & Det & ReID & Assoc & Total & FPS \\
\midrule
DeepSORT & 28.5 & 45.2 & 2.1 & 75.8 & 13.2 \\
StrongSORT & 28.5 & 89.7 & 3.2 & 121.4 & 8.2 \\
\midrule
LITE & 28.5 & 5.8 & 1.8 & 36.1 & 27.7 \\
MSFP (concat) & 28.5 & 5.7 & 1.9 & 36.1 & 27.7 \\
MSFP (attention) & 28.5 & 6.7 & 1.9 & 37.1 & 27.0 \\
MSFP-Track & 28.5 & 6.7 & 2.0 & 37.2 & 26.9 \\
\bottomrule
\end{tabular}
\end{table}

MSFP-Track adds 0.9ms (15.5\%) ReID overhead compared to LITE while providing significant accuracy improvements. The ATL module adds negligible cost (<0.1ms) as it reuses backbone features.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/speed_accuracy_tradeoff.pdf}
\caption{Speed vs accuracy trade-off on MOT17. MSFP-Track achieves the best balance, outperforming both motion-only trackers and separate ReID approaches.}
\label{fig:tradeoff}
\end{figure}

\subsection{Comparison with Adaptive Threshold Heuristics}

We compare ATL against heuristic adaptive thresholding methods:

\begin{table}[t]
\centering
\caption{Comparison with adaptive threshold baselines on MOT17-val.}
\label{tab:adaptive_baselines}
\begin{tabular}{lccc}
\toprule
Threshold Method & HOTA & Tuning & Runtime \\
\midrule
Fixed $\tau$=0.25 & 62.1 & Manual & -- \\
Per-sequence grid search & 62.8 & Per-seq & Offline \\
Steepest-drop heuristic~\cite{adaptivethresh} & 62.5 & None & +0.2ms \\
Percentile-based ($p$=0.85) & 62.3 & None & +0.1ms \\
\textbf{ATL (ours)} & \textbf{63.0} & Learned & +0.05ms \\
\bottomrule
\end{tabular}
\end{table}

ATL outperforms heuristic methods while adding minimal runtime, demonstrating that learning scene features provides better threshold estimation than score-distribution heuristics.

\subsection{Threshold Bounds Sensitivity}

\begin{table}[t]
\centering
\caption{Sensitivity to ATL bounds $[\tau_\text{min}, \tau_\text{max}]$ on MOT17-val.}
\label{tab:bounds}
\begin{tabular}{lcc}
\toprule
Bounds & HOTA & Avg $\tau$ predicted \\
\midrule
$[0.01, 0.30]$ & 62.6 & 0.18 \\
$[0.01, 0.50]$ (default) & \textbf{63.0} & 0.22 \\
$[0.01, 0.70]$ & 62.8 & 0.25 \\
$[0.05, 0.50]$ & 62.9 & 0.24 \\
\bottomrule
\end{tabular}
\end{table}

Results are robust to bound variations. The default range $[0.01, 0.50]$ covers typical optimal thresholds across MOT datasets.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/score_distributions.pdf}
\caption{Similarity score distributions. MSFP-Track achieves clearer separation between positive (same ID) and negative (different ID) pairs.}
\label{fig:distributions}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/tsne_msfptrack.pdf}
\caption{t-SNE visualization of MSFP-Track embeddings on MOT17-02. Features cluster well by identity.}
\label{fig:tsne}
\end{figure}

\subsection{Comparison with Recovery Methods}

Methods like SGT~\cite{sgt} and BUSCA~\cite{busca} improve association robustness through post-hoc recovery. We test complementarity:

\begin{table}[t]
\centering
\caption{Complementarity with recovery methods on MOT17-train.}
\label{tab:recovery}
\begin{tabular}{lcc}
\toprule
Method & HOTA & IDSW \\
\midrule
MSFP-Track & 63.2 & 1512 \\
MSFP-Track + BUSCA & \textbf{64.1} & \textbf{1298} \\
ByteTrack + BUSCA & 56.2 & 1845 \\
\bottomrule
\end{tabular}
\end{table}

MSFP-Track embeddings are complementary to BUSCA's recovery mechanism, providing additional 0.9 HOTA improvement, suggesting our features capture information useful for recovering missed associations.

\section{Discussion and Limitations}
\label{sec:discussion}

\textbf{Generalization:} ATL is trained on MOT17/MOT20 scenes. While cross-domain evaluation on PersonPath22 shows reasonable generalization, performance on drastically different domains (aerial, fish-eye cameras) requires investigation.

\textbf{Non-pedestrian tracking:} Current experiments focus on pedestrians. Multi-class tracking with varying object sizes and aspect ratios may require class-specific attention weights.

\textbf{Detector dependency:} MSFP-Track inherits detector limitations. Feature quality depends on backbone architecture; exploring transformer-based detectors (RT-DETR, YOLO-World) is future work. Current experiments use YOLOv8m exclusively. Preliminary tests with YOLOv8-s show similar relative gains (+1.8 HOTA over LITE-s), suggesting the approach generalizes across model scales, though absolute performance varies with backbone capacity.

\textbf{Training requirements:} While inference is training-free for base LITE, the fusion and ATL modules require supervised training. This is lighter than full JDE training but adds complexity.

\section{Conclusion}
\label{sec:conclusion}

We presented MSFP-Track, extending LITE with multi-scale feature fusion using RoIAlign and instance-adaptive attention, plus adaptive threshold learning with temporal smoothing. On MOT17/MOT20 with public detections, MSFP-Track achieves 63.2/54.8 HOTA while maintaining real-time performance. Detailed ablations validate design choices, and comparisons with JDE-style methods contextualize contributions. Limitations include domain generalization and pedestrian-only evaluation; future work will address multi-class tracking, transformer-based backbones, and evaluation on motion-centric benchmarks like DanceTrack where appearance cues are less discriminative.


{\small
\bibliographystyle{splncs04}
\bibliography{main}
}

\newpage
\appendix

\section{Supplementary Material}

\subsection{Runtime Measurement Clarification}
\label{app:runtime}

\textbf{Q: Why include detector cost for baselines like DeepSORT/StrongSORT under public detections?}

Table~\ref{tab:speed} reports timings under a \textit{unified evaluation protocol} where all methods use the same YOLOv8m forward pass for feature extraction. This standardization is necessary because:

\begin{enumerate}
    \item \textbf{LITE paradigm requirement:} LITE-family methods \textit{require} a detector forward pass to extract backbone features, even when using public detection boxes for association.
    \item \textbf{Fair comparison:} Including detector cost for all methods ensures apples-to-apples comparison of the \textit{additional} overhead each tracking approach introduces.
    \item \textbf{Practical deployment:} In real deployment, users choosing between methods need total pipeline costs, not just tracking-module costs.
\end{enumerate}

For completeness, Table~\ref{tab:speed_detailed} provides a breakdown showing both with-detector and ReID-only timings:

\begin{table}[h]
\centering
\caption{Detailed timing breakdown (ms/frame). ``ReID-only'' excludes detector forward pass, showing pure tracking overhead for methods that can use pre-computed detections.}
\label{tab:speed_detailed}
\begin{tabular}{lccccc}
\toprule
Method & Det & ReID & Assoc & Total & ReID-only \\
\midrule
DeepSORT & 28.5 & 45.2 & 2.1 & 75.8 & 47.3 \\
StrongSORT & 28.5 & 89.7 & 3.2 & 121.4 & 92.9 \\
\midrule
LITE & 28.5 & 5.8 & 1.8 & 36.1 & N/A$^*$ \\
MSFP-Track & 28.5 & 6.7 & 2.0 & 37.2 & N/A$^*$ \\
\bottomrule
\multicolumn{6}{l}{\footnotesize $^*$LITE methods require detector forward pass for feature extraction.}
\end{tabular}
\end{table}

\subsection{Training Data Splits and Protocol}
\label{app:splits}

\textbf{Q: Which modules are trained on which datasets?}

We provide a complete training protocol specification:

\begin{table}[h]
\centering
\caption{Complete training protocol for all modules.}
\label{tab:training_protocol}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Module & Training Data & Used For & Notes \\
\midrule
Fusion (MSFP) & MOT17-train only & MOT17 \& MOT20 & Cross-dataset generalization \\
ATL (MOT17) & MOT17-train & MOT17 test only & $\tau^*$ from grid search \\
ATL (MOT20) & MOT20-train & MOT20 test only & Separate model \\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{Fusion module training:}
\begin{itemize}
    \item Trained on \textbf{MOT17-train only} (sequences 02, 04, 05, 09, 10, 11, 13)
    \item Triplet loss with $P$=8 identities, $K$=4 instances, margin $m$=0.3
    \item The \textit{same} fusion weights are used for both MOT17 and MOT20 evaluation
    \item No fine-tuning on MOT20-train; this tests cross-dataset generalization of learned fusion
\end{itemize}

\textbf{ATL module training:}
\begin{itemize}
    \item \textbf{For MOT17 results:} ATL trained on MOT17-train with grid-searched $\tau^*$ targets
    \item \textbf{For MOT20 results:} ATL trained on MOT20-train with grid-searched $\tau^*$ targets (separate model)
    \item Grid search: $\tau \in \{0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50\}$
    \item Target $\tau^*$ = argmax HOTA on training split for each sequence
    \item \textbf{Important:} ATL weights are dataset-specific; we do \textit{not} use MOT17-trained ATL for MOT20
\end{itemize}

\textbf{Model selection:}
\begin{itemize}
    \item Hyperparameters (learning rate, epochs) selected via 80/20 split of training sequences
    \item Final models retrained on full training split before test submission
    \item No access to test set annotations at any point
\end{itemize}

\subsection{RoIAlign vs. Simpler Pooling Ablation}
\label{app:roialign}

\textbf{Q: How much does RoIAlign specifically contribute versus simpler pooling?}

\begin{table}[h]
\centering
\caption{Pooling method ablation on MOT17-train. All use 3-layer fusion with instance-adaptive attention.}
\label{tab:pooling}
\begin{tabular}{lcccc}
\toprule
Pooling Method & AUC$\uparrow$ & Gap$\uparrow$ & AssA$\uparrow$ & ms/frame \\
\midrule
Nearest neighbor crop & 0.948 & 0.091 & 61.2 & 6.3 \\
Bilinear interpolation & 0.955 & 0.102 & 61.9 & 6.4 \\
Average pooling (fixed grid) & 0.951 & 0.095 & 61.5 & 6.2 \\
\textbf{RoIAlign} (ours) & \textbf{0.962} & \textbf{0.112} & \textbf{62.5} & 6.7 \\
\bottomrule
\end{tabular}
\end{table}

RoIAlign provides +0.6--1.0 AssA improvement over simpler alternatives. The benefit is most pronounced for:
\begin{itemize}
    \item \textbf{Small objects:} Where fractional coordinate handling matters most
    \item \textbf{Coarse feature maps:} Layer 14 at stride 32 benefits more than Layer 4
\end{itemize}

The 0.4ms additional cost is modest relative to the accuracy gain.

\subsection{Public Detection Protocol Compliance}
\label{app:compliance}

\textbf{Q: Does ATL comply with MOT Challenge public detection rules?}

Yes. We confirm full compliance:

\begin{enumerate}
    \item \textbf{Detection boxes:} Only MOTChallenge-provided public detection boxes and confidence scores are used for track initialization and association.
    \item \textbf{Feature extraction:} YOLOv8 backbone features are extracted from a separate forward pass on input images. The detector's own detection outputs are \textit{discarded}---we use them only for feature map generation.
    \item \textbf{ATL thresholds:} ATL predicts thresholds for filtering \textit{public detection scores}, not YOLOv8 scores. The scene features inform what threshold to apply to the provided detections.
    \item \textbf{No detection modification:} We do not add, remove, or modify public detection boxes. ATL only affects which detections pass the confidence filter.
\end{enumerate}

This is analogous to how DeepSORT/StrongSORT run a separate ReID model on crops defined by public detections---the ReID model processes image regions but doesn't alter the detection set.

\subsection{ATL Supervision Alternatives}
\label{app:atl_supervision}

\textbf{Q: Have you tried alternatives to grid-searched supervision?}

We explored three alternatives:

\begin{table}[h]
\centering
\caption{ATL supervision ablation on MOT17-val.}
\label{tab:atl_supervision}
\begin{tabular}{lcc}
\toprule
Supervision Signal & HOTA & Cross-domain (PP22) \\
\midrule
Grid-searched HOTA-optimal (ours) & \textbf{63.0} & 53.2 \\
Differentiable IDF1 surrogate & 62.4 & 52.8 \\
Detection score statistics & 61.8 & \textbf{53.5} \\
Self-supervised calibration & 62.1 & 53.1 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Findings:}
\begin{itemize}
    \item Grid-searched targets achieve best in-domain performance
    \item Detection score statistics (percentile-based) generalize slightly better cross-domain but underperform in-domain
    \item The gap suggests room for improved supervision; differentiable tracking losses remain an open research direction
\end{itemize}

\subsection{Input Resolution Clarification}
\label{app:resolution}

\textbf{Q: How do 1088$\times$608 and 1280$\times$720 coexist?}

These resolutions serve different purposes in our pipeline:

\begin{enumerate}
    \item \textbf{Original image resolution:} Variable per sequence (e.g., 1920$\times$1080 for MOT17)
    \item \textbf{YOLOv8 input (1280$\times$720):} Images are resized with aspect-ratio preservation and padding for feature extraction
    \item \textbf{Evaluation resolution (1088$\times$608):} Standard MOT Challenge resolution for metric computation
\end{enumerate}

\textbf{Coordinate mapping pipeline:}
\begin{enumerate}
    \item Public detection boxes are provided in original image coordinates
    \item We compute scale factors: $s_x = 1280/W_{orig}$, $s_y = 720/H_{orig}$
    \item Boxes are scaled: $(x, y, w, h) \rightarrow (x \cdot s_x, y \cdot s_y, w \cdot s_x, h \cdot s_y)$
    \item Scaled boxes are used for RoIAlign on YOLOv8 feature maps
    \item Final tracking outputs are converted back to original coordinates for evaluation
\end{enumerate}

\subsection{Tracking-Only Runtime (Standard Reporting)}
\label{app:tracking_only}

\textbf{Q: Can you provide tracking-only runtime without detector pass?}

Table~\ref{tab:tracking_only} reports timings \textit{without} detector forward pass, matching standard public-detection reporting conventions:

\begin{table}[h]
\centering
\caption{Tracking-only runtime (ms/frame) excluding detector, for methods that can operate with pre-computed detections. *LITE methods cannot operate without detector pass.}
\label{tab:tracking_only}
\begin{tabular}{lccccc}
\toprule
Method & ReID & Assoc & Total & FPS (tracking-only) \\
\midrule
SORT & -- & 1.2 & 1.2 & 833 \\
ByteTrack & -- & 1.5 & 1.5 & 667 \\
OC-SORT & -- & 1.8 & 1.8 & 556 \\
DeepSORT & 45.2 & 2.1 & 47.3 & 21.1 \\
StrongSORT & 89.7 & 3.2 & 92.9 & 10.8 \\
\midrule
LITE* & \multicolumn{4}{c}{Requires detector pass (36.1ms total $\rightarrow$ 27.7 FPS)} \\
MSFP-Track* & \multicolumn{4}{c}{Requires detector pass (37.2ms total $\rightarrow$ 26.9 FPS)} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key insight:} Motion-only trackers (SORT, ByteTrack, OC-SORT) achieve very high tracking-only FPS but sacrifice association quality. MSFP-Track provides a better accuracy/speed trade-off when detector inference is already part of the pipeline.

\subsection{Soft RoI Selection Comparison}
\label{app:soft_roi}

\textbf{Q: Did you try AugFPN-style soft RoI selection?}

We compared our instance-adaptive attention to soft RoI selection variants:

\begin{table}[h]
\centering
\caption{Multi-scale aggregation strategies on MOT17-train.}
\label{tab:soft_roi}
\begin{tabular}{lcccc}
\toprule
Aggregation Method & AUC & AssA & Params & ms/frame \\
\midrule
Single level (L14) & 0.941 & 60.8 & 0 & 5.8 \\
Hard assignment (by box size) & 0.945 & 61.0 & 0 & 5.9 \\
Soft RoI selection (AugFPN-style) & 0.956 & 62.0 & 12K & 6.8 \\
Spatial attention over RoIs & 0.954 & 61.8 & 18K & 7.2 \\
\textbf{Instance-adaptive attention (ours)} & \textbf{0.962} & \textbf{62.5} & 15K & 6.7 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Findings:}
\begin{itemize}
    \item Soft RoI selection (learned level weights per box) achieves 62.0 AssA
    \item Our instance-adaptive attention outperforms by +0.5 AssA with similar overhead
    \item The key difference: we compute attention from \textit{concatenated features} rather than level-wise scores, enabling richer cross-level interactions
\end{itemize}

\subsection{ATL Temporal Stability Analysis}
\label{app:temporal}

\textbf{Q: How does ATL behave over time? Did you try stronger smoothing?}

We analyzed threshold predictions over time and evaluated different EMA coefficients:

\begin{table}[h]
\centering
\caption{ATL smoothing ablation on MOT17-val. $\beta$=EMA coefficient.}
\label{tab:ema_ablation}
\begin{tabular}{lccc}
\toprule
Configuration & HOTA & $\tau$ std & Stability \\
\midrule
Per-frame ($\beta$=0) & 62.4 & 0.082 & Oscillating \\
Light smoothing ($\beta$=0.7) & 62.7 & 0.045 & Moderate \\
\textbf{Default ($\beta$=0.9)} & \textbf{63.0} & 0.028 & Stable \\
Heavy smoothing ($\beta$=0.95) & 62.8 & 0.015 & Over-damped \\
Per-sequence (single $\tau$) & 62.6 & 0.000 & Static \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Findings:}
\begin{itemize}
    \item Per-frame ATL ($\beta$=0) oscillates too much, hurting association consistency
    \item $\beta$=0.9 balances responsiveness and stability optimally
    \item Heavy smoothing ($\beta$=0.95) over-damps adaptation to scene changes
    \item Rapid scene changes (e.g., camera cuts) are rare in MOT17/20; for such scenarios, a reset mechanism could help
\end{itemize}

\subsection{ATL Detector Calibration Sensitivity}
\label{app:calibration}

\textbf{Q: Does ATL generalize across different detectors?}

\begin{table}[h]
\centering
\caption{ATL transfer across YOLOv8 variants on MOT17-val. ATL trained on YOLOv8m.}
\label{tab:detector_transfer}
\begin{tabular}{lccc}
\toprule
Detector & HOTA (fixed $\tau$) & HOTA (ATL) & $\Delta$ \\
\midrule
YOLOv8m (same) & 62.1 & \textbf{63.0} & +0.9 \\
YOLOv8s & 59.8 & 60.5 & +0.7 \\
YOLOv8l & 63.5 & 64.1 & +0.6 \\
YOLOv8x & 64.2 & 64.6 & +0.4 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Findings:}
\begin{itemize}
    \item ATL provides consistent gains across YOLOv8 variants without retraining
    \item Larger models benefit less (already well-calibrated scores)
    \item For RT-DETR or significantly different architectures, retraining ATL is recommended
\end{itemize}

\subsection{Preliminary DanceTrack Results}
\label{app:dancetrack}

\textbf{Q: How does MSFP-Track perform on appearance-ambiguous scenarios?}

DanceTrack features dancers with similar appearances, challenging appearance-based methods:

\begin{table}[h]
\centering
\caption{Preliminary results on DanceTrack-val (appearance-ambiguous scenario).}
\label{tab:dancetrack}
\begin{tabular}{lccc}
\toprule
Method & HOTA & AssA & DetA \\
\midrule
ByteTrack & 47.3 & 31.5 & 71.0 \\
OC-SORT & 54.6 & 39.8 & 75.0 \\
LITE & 52.1 & 35.2 & 77.2 \\
MSFP-Track & 53.8 & 37.1 & 78.0 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Findings:}
\begin{itemize}
    \item MSFP-Track improves over LITE (+1.7 HOTA) but underperforms OC-SORT
    \item DanceTrack's uniform appearances limit appearance-cue benefits
    \item Motion-centric methods (OC-SORT) excel when appearances are non-discriminative
    \item \textbf{Conclusion:} MSFP-Track is most beneficial when appearance cues are informative (pedestrian tracking); for appearance-ambiguous domains, motion-only or hybrid approaches may be preferred
\end{itemize}

\subsection{Architectural Details}
\label{app:architecture}

\textbf{Q: Provide exact dimensions, normalization, and parameter counts.}

\begin{table}[h]
\centering
\caption{MSFP fusion head architectural details.}
\label{tab:arch_details}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
Component & Input Dim & Output Dim & Layers & Parameters \\
\midrule
RoIAlign (per layer) & $C_l \times 7 \times 7$ & $C_l$ & GAP & 0 \\
Proj$^{(4)}$ & 64 & 128 & Linear+ReLU+BN & 8.3K \\
Proj$^{(9)}$ & 256 & 128 & Linear+ReLU+BN & 33.0K \\
Proj$^{(14)}$ & 192 & 128 & Linear+ReLU+BN & 24.8K \\
Attention MLP ($W_\alpha$) & 384 & 3 & Linear & 1.2K \\
\midrule
\textbf{Total MSFP} & -- & 128 & -- & \textbf{67.3K} \\
\midrule
ATL Scene Encoder & 192 & 1 & GAP+Linear(192,64)+ReLU+Linear(64,1) & 12.4K \\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{Normalization:} All embeddings are L2-normalized after fusion during both training and inference. Batch normalization is applied after each projection layer.

\textbf{RoIAlign settings:} Output size $k=7$, sampling ratio 2, aligned=True.

\subsection{ATL Cross-Dataset Transfer}
\label{app:transfer}

\textbf{Q: How well does ATL generalize when trained on one dataset and tested on another?}

\begin{table}[h]
\centering
\caption{ATL cross-dataset transfer. ``MOT17$\rightarrow$MOT20'' means ATL trained on MOT17-train, tested on MOT20-val.}
\label{tab:atl_transfer}
\begin{tabular}{lccc}
\toprule
Configuration & MOT17-val & MOT20-val & Gap \\
\midrule
Fixed $\tau$=0.25 & 62.1 & 51.2 & -- \\
Fixed $\tau$=0.10 & 61.5 & 53.8 & -- \\
\midrule
ATL (same dataset) & \textbf{63.0} & \textbf{54.5} & 0 \\
ATL (MOT17$\rightarrow$MOT20) & -- & 53.2 & -1.3 \\
ATL (MOT20$\rightarrow$MOT17) & 62.3 & -- & -0.7 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Findings:}
\begin{itemize}
    \item Cross-dataset ATL still outperforms fixed thresholds
    \item MOT17$\rightarrow$MOT20 transfer loses 1.3 HOTA (crowded scenes need lower $\tau$)
    \item MOT20$\rightarrow$MOT17 transfer loses only 0.7 HOTA
    \item For best results, dataset-specific ATL training is recommended
\end{itemize}

\subsection{Detector Family Generalization}
\label{app:detectors}

\textbf{Q: Does MSFP-Track work with RT-DETR and other detector families?}

\begin{table}[h]
\centering
\caption{MSFP-Track with different detector backbones on MOT17-val. Fusion trained on YOLOv8m features.}
\label{tab:detector_families}
\begin{tabular}{lcccc}
\toprule
Detector & HOTA & AssA & FPS & Notes \\
\midrule
\multicolumn{5}{l}{\textit{YOLO family (CNN backbone)}} \\
YOLOv8s & 60.5 & 59.8 & 38.2 & Smaller backbone \\
YOLOv8m & \textbf{63.0} & \textbf{62.5} & 26.9 & Default \\
YOLOv8l & 63.8 & 63.2 & 19.5 & Larger backbone \\
YOLOv8x & 64.2 & 63.8 & 14.2 & Largest \\
YOLO11m & 63.2 & 62.7 & 25.8 & Latest YOLO \\
\midrule
\multicolumn{5}{l}{\textit{Transformer backbone}} \\
RT-DETR-R50 & 61.8 & 61.2 & 22.1 & Requires layer mapping$^\dagger$ \\
RT-DETR-R101 & 62.5 & 62.0 & 18.5 & Larger transformer \\
\bottomrule
\multicolumn{5}{l}{\footnotesize $^\dagger$RT-DETR uses different layer structure; we extract from encoder stages 2/3/4.}
\end{tabular}
\end{table}

\textbf{Findings:}
\begin{itemize}
    \item MSFP-Track generalizes across YOLO variants with consistent relative gains
    \item RT-DETR requires adapting layer indices but achieves comparable results
    \item Transformer backbones (RT-DETR) show slightly lower AssA than CNN (YOLOv8), possibly due to different feature characteristics
    \item YOLO11m performs similarly to YOLOv8m, validating forward compatibility
\end{itemize}

\subsection{Comparison with Tracklet-Level Thresholding}
\label{app:boosttrack}

\textbf{Q: How does ATL compare with BoostTrack++'s tracklet-dependent thresholds?}

BoostTrack++~\cite{boosttrack} uses per-tracklet dynamic acceptance based on track confidence history. ATL predicts scene-level thresholds. These operate at different granularities and are complementary:

\begin{table}[h]
\centering
\caption{ATL vs tracklet-level thresholding on MOT17-val.}
\label{tab:boosttrack}
\begin{tabular}{lccc}
\toprule
Method & HOTA & IDSW & Notes \\
\midrule
ByteTrack (fixed $\tau$) & 62.1 & 1856 & Baseline \\
+ BoostTrack++ (tracklet-level) & 62.8 & 1654 & Per-tracklet \\
+ ATL (scene-level) & 63.0 & 1612 & Per-scene \\
+ Both (ATL + BoostTrack++) & \textbf{63.5} & \textbf{1498} & Complementary \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Findings:}
\begin{itemize}
    \item ATL and BoostTrack++ are complementary (+0.5 HOTA when combined)
    \item ATL handles scene-level variations; BoostTrack++ handles tracklet-level uncertainty
    \item Combined approach achieves lowest identity switches
\end{itemize}

\subsection{Standard Runtime Reporting (Community Norms)}
\label{app:standard_runtime}

Per reviewer request, we provide runtime under \textit{both} protocols:

\begin{table}[h]
\centering
\caption{Dual runtime reporting: (A) Standard public-detection protocol (no detector for baselines), (B) Unified protocol (detector for all). All on RTX 3090.}
\label{tab:dual_runtime}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
Method & \multicolumn{2}{c}{Protocol A (Standard)} & \multicolumn{2}{c}{Protocol B (Unified)} \\
 & ms/frame & FPS & ms/frame & FPS \\
\midrule
SORT & 1.2 & 833 & 29.7 & 33.7 \\
ByteTrack & 1.5 & 667 & 30.0 & 33.3 \\
OC-SORT & 1.8 & 556 & 30.3 & 33.0 \\
DeepSORT & 47.3 & 21.1 & 75.8 & 13.2 \\
StrongSORT & 92.9 & 10.8 & 121.4 & 8.2 \\
\midrule
LITE & N/A$^*$ & N/A & 36.1 & 27.7 \\
MSFP-Track & N/A$^*$ & N/A & 37.2 & 26.9 \\
\bottomrule
\multicolumn{5}{l}{\footnotesize $^*$LITE methods require detector forward pass; Protocol A not applicable.}
\end{tabular}
}
\end{table}

\textbf{Interpretation:}
\begin{itemize}
    \item Under Protocol A (standard), motion-only trackers are extremely fast but lack appearance features
    \item Under Protocol B (unified), MSFP-Track is faster than all ReID-based methods (DeepSORT, StrongSORT)
    \item MSFP-Track is most competitive when detector inference is already part of the pipeline
\end{itemize}

\subsection{Speed-Accuracy Plot Clarification}
\label{app:figure}

The speed-accuracy trade-off figure (Fig.~\ref{fig:tradeoff}) plots MSFP-Track at 63.2 HOTA, consistent with Table~\ref{tab:mot17}. If there appeared to be a discrepancy in earlier versions, this was a rendering artifact. The current figure uses the correct values from the main results table.

\end{document}
