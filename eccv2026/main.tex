\documentclass[runningheads]{llncs}

% ECCV package
\usepackage[review,year=2026,ID=*****]{eccv}

% Other packages
\usepackage{eccvabbrv}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage[accsupp]{axessibility}

% Hyperref
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}

% ORCID support
\usepackage{orcidlink}

\begin{document}

\title{LITE++: Multi-Scale Feature Fusion with Adaptive Thresholds for Real-Time Multi-Object Tracking}

\titlerunning{LITE++: Multi-Scale MOT}

\author{Anonymous Authors}
\authorrunning{Anonymous}

\institute{Anonymous Institution}

\maketitle

\begin{abstract}
Multi-object tracking (MOT) in real-time applications requires a careful balance between accuracy and computational efficiency. The LITE paradigm demonstrated that appearance features can be extracted directly from object detector backbones without separate re-identification (ReID) models, achieving significant speedups. However, LITE is limited to single-layer features and requires manual threshold tuning across different scenarios. We propose \textbf{LITE++}, which extends LITE with three key innovations: (1) \textbf{Multi-Scale Feature Pyramid Fusion} that extracts and combines features from multiple backbone layers for richer representations; (2) \textbf{Adaptive Threshold Learning} that predicts scene-specific confidence thresholds, eliminating manual tuning; and (3) three fusion strategies (concatenation, attention-weighted, and channel-adaptive) with ablation analysis. Experiments on MOT17 and MOT20 benchmarks show that LITE++ improves association accuracy while maintaining real-time performance, achieving a better accuracy-speed trade-off than existing methods.

\keywords{Multi-Object Tracking \and Re-Identification \and Feature Fusion \and Real-Time Tracking}
\end{abstract}

\section{Introduction}
\label{sec:intro}

Multi-object tracking (MOT) is a fundamental computer vision task with applications in autonomous driving, video surveillance, and robotics. The tracking-by-detection paradigm~\cite{sort,deepsort} has emerged as the dominant approach, where objects are first detected and then associated across frames using motion and appearance cues.

Traditional appearance-based trackers like DeepSORT~\cite{deepsort} and StrongSORT~\cite{strongsort} rely on separate re-identification (ReID) models to extract discriminative features. While effective, these approaches incur significant computational overhead, limiting real-time applicability.

The LITE paradigm~\cite{lite} introduced a novel approach: extracting appearance features directly from intermediate layers of the object detector during inference, eliminating the need for separate ReID models. This achieves 2-10$\times$ speedup while maintaining competitive accuracy. However, LITE has two key limitations:

\begin{enumerate}
    \item \textbf{Single-layer features}: LITE extracts features from only one backbone layer, missing multi-scale information crucial for handling objects at different scales and occlusion levels.
    \item \textbf{Manual threshold tuning}: The optimal detection confidence threshold varies significantly across datasets (0.25 for MOT17, 0.05 for MOT20), requiring manual tuning for each deployment scenario.
\end{enumerate}

We propose \textbf{LITE++} to address these limitations with three contributions:

\begin{itemize}
    \item \textbf{Multi-Scale Feature Pyramid Fusion (MSFP)}: We extract features from multiple backbone layers (early, mid, late) and fuse them using learnable attention weights, capturing both fine-grained details and semantic information.

    \item \textbf{Adaptive Threshold Learning (ATL)}: We introduce a scene encoder that predicts optimal confidence thresholds based on scene characteristics, eliminating manual tuning while adapting to varying crowd densities and lighting conditions.

    \item \textbf{Comprehensive ablation}: We compare three fusion strategies---concatenation, attention-weighted, and channel-adaptive (SE-style)---providing insights into effective multi-scale feature combination for MOT.
\end{itemize}

\section{Related Work}
\label{sec:related}

\subsection{Tracking-by-Detection}

The tracking-by-detection paradigm has dominated MOT research. SORT~\cite{sort} introduced a simple yet effective approach using Kalman filtering and Hungarian matching. DeepSORT~\cite{deepsort} extended this with appearance features from a separately trained ReID model. Recent methods like ByteTrack~\cite{bytetrack}, OC-SORT~\cite{ocsort}, and BoTSORT~\cite{botsort} focus on improving association strategies while maintaining real-time performance.

\subsection{ReID in Multi-Object Tracking}

Appearance-based association relies on discriminative features to match detections across frames. Traditional approaches use separate ReID models~\cite{strongsort,botsort}, adding computational overhead. LITE~\cite{lite} pioneered zero-cost ReID by extracting features from detector backbones, achieving significant speedups.

\subsection{Multi-Scale Feature Fusion}

Feature pyramid networks (FPN)~\cite{fpn} demonstrated the importance of multi-scale features for object detection. In tracking, combining features from multiple scales can improve handling of varying object sizes and occlusions. Our work applies this insight to ReID feature extraction within the LITE framework.

\section{Method}
\label{sec:method}

\subsection{Overview}

LITE++ extends the LITE paradigm with multi-scale feature extraction and adaptive threshold prediction. Given an input image $I$ and detected bounding boxes $\{b_i\}_{i=1}^N$, LITE++ extracts appearance features $\{f_i\}_{i=1}^N$ from multiple backbone layers and predicts scene-adaptive confidence threshold $\tau$.

\subsection{Multi-Scale Feature Pyramid Fusion}

We extract features from three backbone layers of the YOLO detector:
\begin{itemize}
    \item \textbf{Layer 4} (early): High spatial resolution, captures fine-grained details
    \item \textbf{Layer 9} (mid): Intermediate resolution, balances detail and semantics
    \item \textbf{Layer 14} (late): Low spatial resolution, rich semantic features
\end{itemize}

For each detected box $b_i = (x_1, y_1, x_2, y_2)$, we map coordinates to each feature map resolution and extract region features via spatial average pooling:

\begin{equation}
    f_i^{(l)} = \frac{1}{|R_i^{(l)}|} \sum_{(h,w) \in R_i^{(l)}} F^{(l)}_{:,h,w}
\end{equation}

where $F^{(l)}$ is the feature map from layer $l$ and $R_i^{(l)}$ is the region corresponding to box $b_i$.

\subsubsection{Fusion Strategies}

We investigate three fusion strategies:

\textbf{Concatenation:} Simple concatenation followed by MLP projection:
\begin{equation}
    f_i = \text{MLP}([f_i^{(4)}; f_i^{(9)}; f_i^{(14)}])
\end{equation}

\textbf{Attention-Weighted:} Learned attention weights $\alpha^{(l)}$ for each layer:
\begin{equation}
    f_i = \sum_l \alpha^{(l)} \cdot \text{Proj}^{(l)}(f_i^{(l)}), \quad \alpha = \text{softmax}(w)
\end{equation}

\textbf{Channel-Adaptive (SE-style):} Squeeze-and-Excitation attention~\cite{senet}:
\begin{equation}
    f_i = \text{Proj}(\text{SE}([f_i^{(4)}; f_i^{(9)}; f_i^{(14)}]))
\end{equation}

\subsection{Adaptive Threshold Learning}

The optimal detection confidence threshold varies significantly across scenes. We propose a scene encoder that predicts thresholds based on global scene features:

\begin{equation}
    \tau = \sigma(\text{MLP}(\text{GAP}(F^{(14)}))) \cdot (\tau_{max} - \tau_{min}) + \tau_{min}
\end{equation}

where GAP denotes global average pooling and $\sigma$ is the sigmoid function. The output is bounded to $[\tau_{min}, \tau_{max}] = [0.01, 0.50]$.

The scene encoder captures:
\begin{itemize}
    \item \textbf{Crowd density}: High-density scenes benefit from lower thresholds
    \item \textbf{Occlusion patterns}: Occluded objects require lower confidence thresholds
    \item \textbf{Domain characteristics}: Traffic vs. retail scenes have different optimal thresholds
\end{itemize}

\subsection{Training}

The threshold module is trained with supervised loss using grid-search optimal thresholds as targets:
\begin{equation}
    \mathcal{L}_\tau = \text{MSE}(\tau_{pred}, \tau^*)
\end{equation}

The feature fusion module is trained end-to-end with the tracker using association loss.

\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\textbf{Datasets:} We evaluate on MOT17~\cite{mot17} and MOT20~\cite{mot20} benchmarks. MOT17 contains medium-density scenes while MOT20 focuses on crowded scenarios.

\textbf{Metrics:} We use HOTA (Higher Order Tracking Accuracy) as the primary metric, along with AssA (Association Accuracy), DetA (Detection Accuracy), and IDF1.

\textbf{Implementation:} We use YOLOv8m as the base detector. All experiments run on a single NVIDIA RTX 3090 GPU. Feature dimension is 128 for multi-layer variants.

\subsection{Comparison with State-of-the-Art}

\begin{table}[t]
\centering
\caption{Comparison with state-of-the-art methods on MOT17 test set.}
\label{tab:sota}
\begin{tabular}{lccccc}
\toprule
Method & HOTA $\uparrow$ & AssA $\uparrow$ & DetA $\uparrow$ & IDF1 $\uparrow$ & FPS $\uparrow$ \\
\midrule
DeepSORT~\cite{deepsort} & 43.7 & 42.5 & 45.1 & 58.2 & 13.7 \\
StrongSORT~\cite{strongsort} & 41.7 & 40.8 & 42.8 & 55.3 & 5.1 \\
ByteTrack~\cite{bytetrack} & 43.8 & 41.2 & 46.6 & 57.1 & 29.7 \\
OC-SORT~\cite{ocsort} & 44.1 & 42.8 & 45.5 & 58.9 & 28.5 \\
\midrule
LITE:DeepSORT~\cite{lite} & 43.0 & 41.9 & 44.2 & 57.5 & 28.3 \\
\textbf{LITE++} (ours) & \textbf{44.5} & \textbf{43.6} & \textbf{45.6} & \textbf{59.8} & 26.1 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Studies}

\subsubsection{Feature Fusion Strategy}

\begin{table}[t]
\centering
\caption{Ablation on fusion strategies. AUC measures re-identification discriminability.}
\label{tab:fusion}
\begin{tabular}{lccc}
\toprule
Fusion Strategy & AUC $\uparrow$ & Pos-Neg Gap $\uparrow$ & Feature Dim \\
\midrule
Single Layer (LITE) & 0.9962 & 0.0275 & 64 \\
Concat + MLP & 0.9945 & 0.0498 & 128 \\
Attention-Weighted & \textbf{0.9972} & 0.0535 & 128 \\
Channel-Adaptive (SE) & 0.9928 & \textbf{0.0586} & 128 \\
\bottomrule
\end{tabular}
\end{table}

The attention-weighted fusion achieves the highest AUC (0.9972), indicating superior discriminability for re-identification. The channel-adaptive approach achieves the largest positive-negative gap (0.0586), suggesting better separation between same-identity and different-identity pairs.

\subsubsection{Layer Combination}

\begin{table}[t]
\centering
\caption{Ablation on layer combinations for feature extraction.}
\label{tab:layers}
\begin{tabular}{lcc}
\toprule
Layers & AUC $\uparrow$ & Pos-Neg Gap $\uparrow$ \\
\midrule
Layer 14 only (baseline) & 0.9962 & 0.0275 \\
Layer 9 + 14 & 0.9968 & 0.0412 \\
Layer 4 + 14 & 0.9955 & 0.0389 \\
Layer 4 + 9 + 14 & \textbf{0.9972} & \textbf{0.0535} \\
\bottomrule
\end{tabular}
\end{table}

Using all three layers provides the best performance, confirming that multi-scale features capture complementary information beneficial for appearance-based association.

\subsubsection{Adaptive Threshold}

\begin{table}[t]
\centering
\caption{Impact of adaptive threshold learning.}
\label{tab:threshold}
\begin{tabular}{lcc}
\toprule
Configuration & MOT17 HOTA & MOT20 HOTA \\
\midrule
Fixed ($\tau=0.25$) & 44.1 & 38.2 \\
Fixed ($\tau=0.05$) & 42.8 & 42.5 \\
Grid-search optimal & 44.3 & 42.7 \\
Adaptive (ours) & \textbf{44.5} & \textbf{42.8} \\
\bottomrule
\end{tabular}
\end{table}

The adaptive threshold module achieves performance comparable to grid-search optimal thresholds while eliminating the need for manual tuning per dataset.

\subsection{Speed Analysis}

\begin{table}[t]
\centering
\caption{Speed comparison of ReID variants (ms per frame).}
\label{tab:speed}
\begin{tabular}{lcc}
\toprule
Method & ReID Time (ms) & Overhead \\
\midrule
DeepSORT (separate model) & 45.2 & +328\% \\
StrongSORT (BoT-S50) & 89.7 & +652\% \\
LITE (single layer) & 10.6 & baseline \\
LITE++ (3 layers, attention) & 15.8 & +49\% \\
LITE++ (3 layers, concat) & 13.2 & +25\% \\
\bottomrule
\end{tabular}
\end{table}

LITE++ adds minimal overhead (25-49\%) compared to single-layer LITE while significantly improving association accuracy. This is far more efficient than traditional ReID models which add 300-650\% overhead.

\section{Conclusion}
\label{sec:conclusion}

We presented LITE++, extending the LITE paradigm with multi-scale feature fusion and adaptive threshold learning for real-time multi-object tracking. Our multi-scale feature pyramid fusion captures complementary information from different backbone layers, improving re-identification discriminability. The adaptive threshold module eliminates manual threshold tuning while maintaining performance across diverse scenarios. Experiments demonstrate that LITE++ achieves state-of-the-art accuracy-speed trade-offs, making it suitable for real-time deployment.

\textbf{Limitations and Future Work:} Current experiments focus on pedestrian tracking. Future work will extend to multi-class tracking and explore transformer-based backbones (RT-DETR, YOLO-World).


{\small
\bibliographystyle{splncs04}
\bibliography{main}
}

\end{document}
